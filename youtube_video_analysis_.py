# -*- coding: utf-8 -*-
"""Youtube video analysis .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qqhyFUZ40rXFwQZWasUTrMUGwqAxNGOh

Importing libraries
"""

import numpy as np
import pandas as pd
import datetime
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import seaborn as sns

"""Importing Data"""

youtube_india_data=pd.read_csv('/content/drive/MyDrive/Project work/IN_youtube_trending_data.csv')

print(youtube_india_data.shape)

"""Structure of the data"""

youtube_india_data.info()

"""Cleaning Data"""

youtube_india_data['trending_date']=pd.to_datetime(youtube_india_data['trending_date'],infer_datetime_format=True)

youtube_india_data['publishedAt']=pd.to_datetime(youtube_india_data['publishedAt'],infer_datetime_format=True)

"""Removing null data"""

youtube_india_data.dropna(subset=['channelTitle'],inplace=True)

"""Filtering Data"""

youtube_last_yr=youtube_india_data.loc[youtube_india_data['trending_date'].dt.year==2022]

"""Checking the first and last date in the data."""

print('Data from',min(youtube_last_yr['trending_date']),'to',max(youtube_last_yr['trending_date']))

"""Extracting date"""

youtube_last_yr['day']=youtube_last_yr['trending_date'].dt.day
youtube_last_yr['month']=youtube_last_yr['trending_date'].dt.month
youtube_last_yr['year']=youtube_last_yr['trending_date'].dt.year

"""Category description"""

youtube_cat_in=pd.read_json('/content/drive/MyDrive/Project work/IN_category_id.json')

""" LinearRegression"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

data = pd.read_csv("/content/drive/MyDrive/Project data set/IN_youtube_trending_data.csv")
data = data.dropna(subset=['trending_date', 'publishedAt'])


data['likes'] = pd.to_numeric(data['likes'])
data['dislikes'] = pd.to_numeric(data['dislikes'])


lr = LinearRegression()
#lr.fit(np.array(data['likes']).reshape((-1, 1)), np.array(data['dislikes']).reshape((-1, 1)))


plt.scatter(data['likes'], data['dislikes'])
#plt.plot(data['likes'], lr.predict(np.array(data['likes']).reshape((-1, 1))), color='red', linewidth=2)
plt.show()

"""LogisticRegression

"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/Project data set/IN_youtube_trending_data.csv")

# Define features (X) and target variable (y)
X = data[['likes', 'dislikes']]
y = data['view_count']  # Assuming 'views' is the target variable, change it accordingly

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create a linear regression model
linear_reg_model = LinearRegression()

# Train the model
linear_reg_model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = linear_reg_model.predict(X_train_scaled)

# Evaluate the model using regression metrics
mae = mean_absolute_error(y_train, y_pred)
mse = mean_squared_error(y_train, y_pred)
r2 = r2_score(y_train, y_pred)

plt.scatter(y_train, y_pred)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

print(f'Mean Absolute Error: {mae:.2f}')
print(f'Mean Squared Error: {mse:.2f}')
print(f'R-squared: {r2:.2f}')

"""Parsing category Id"""

id=[]
cat=[]
for t in youtube_cat_in['items']:
    for x,y in t.items():
        if x=='id':
            id.append(y)
            #print(y)#category id
        if x=='snippet':
            for a,b in y.items():
                if a=='title':
                    cat.append(b)
                    #print(b)#category name
                    break
cat_dict={
    'id':id,
    'category':cat
}
category_id=pd.DataFrame(cat_dict)
category_id['id']=category_id['id'].astype('int64')
category_id

from google.colab import drive
drive.mount('/content/drive')

"""Analysis

1. Which type of videos are trending the most in the last year?
"""

#per year which type of video are more in trending
trend_yr=youtube_last_yr[['video_id','categoryId','month']]
video_trend_yr=pd.DataFrame(trend_yr.groupby(by='categoryId').video_id.nunique().sort_values(ascending=False)).merge(category_id,how='inner',left_on='categoryId',right_on='id').rename(columns={'video_id':'video_count'})
sns.barplot(x=video_trend_yr['video_count'],y=video_trend_yr['category'],palette='viridis')

"""2. Which channel has their video trending the most?"""

#how many trending videos published by a channel
top5_channel=pd.DataFrame(youtube_last_yr.groupby(by='channelTitle').video_id.nunique().sort_values(ascending=False)).rename(columns={'video_id':'video_count'}).head(10)
sns.color_palette("hls", 100)
sns.barplot(x=top5_channel['video_count'],y=top5_channel.index,palette='viridis')

"""3. Top 3 channel whose videos end up on trending page by each month"""

#how many trending videos published per week/month by channel
chan_trend_month=pd.DataFrame(youtube_last_yr.groupby(['month','channelTitle']).video_id.nunique().sort_values(ascending=False)).rename(columns={'video_id':'video_count'}).reset_index()
for m in range(1,13):
    print(chan_trend_month.loc[chan_trend_month.month==m].head(3))

"""4. Which type of videos take the shortest time to get on trending page after publishing?"""

#how many days after publishing video goes to trending(cat)
df1=youtube_last_yr[['video_id','publishedAt','channelTitle','categoryId']]
df1=df1[~df1.duplicated()]
df2=youtube_last_yr['trending_date']
df1=df1.join(df2)
df1=df1.join([df1['trending_date'].dt.date-df1['publishedAt'].dt.date])

df1=df1.rename(columns={0:'deltatrend'})
cat_deltatrend=pd.DataFrame(df1.groupby(by='categoryId').deltatrend.mean().sort_values(ascending=True)).merge(category_id,how='left',left_on='categoryId',right_on='id').dropna()
sns.barplot(x=cat_deltatrend['deltatrend']/np.timedelta64(1, 'D'),y=cat_deltatrend['category'],palette='crest')

"""5. Which channel's videos take the shortest time to get on trending page after publishing"""

#how many days after publishing video goes to trending(channel)
df1.groupby(by='channelTitle').deltatrend.mean().sort_values(ascending=True).head(10)

"""6. Which type of video remained longest on trending page?"""

#which category of video remained longest trending
df11=youtube_last_yr.groupby(['video_id','channelTitle','categoryId']).apply(lambda x:max(x.trending_date)-min(x.trending_date))
df11=pd.DataFrame(df11).rename(columns={0:'trending_days'})
cat_remain_trend=pd.DataFrame(df11.groupby(['categoryId']).trending_days.mean().sort_values(ascending=False)).merge(category_id,how='left',left_on='categoryId',right_on='id')
sns.barplot(x=cat_remain_trend['trending_days']/np.timedelta64(1, 'D'),y=cat_remain_trend['category'],palette='viridis')

"""7. Which channel's videos remained the longest on trending page?"""

#which channel has videos remained longest trending
chan_remain_trend=pd.DataFrame(df11.groupby(['channelTitle']).trending_days.mean().sort_values(ascending=False)).head(10)
sns.barplot(x=chan_remain_trend['trending_days']/np.timedelta64(1, 'D'),y=chan_remain_trend.index,palette='viridis')